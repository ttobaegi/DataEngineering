{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# Set up SparkContext\n",
    "conf = SparkConf().setMaster('local').setAppName('Test')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-MT5F0AJ:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x220d5f9ff60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Load Data into Dataframes : CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\JIIN\\\\OneDrive\\\\Documents\\\\Github\\\\2021_Python\\\\Spark_SQL\\\\data'\n",
    "file_path = data_path + '\\\\location_temp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event_date='03/04/2019 19:48:06', location_id='loc0', temp_celcius='29'),\n",
       " Row(event_date='03/04/2019 19:53:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 19:58:06', location_id='loc0', temp_celcius='28'),\n",
       " Row(event_date='03/04/2019 20:03:06', location_id='loc0', temp_celcius='30'),\n",
       " Row(event_date='03/04/2019 20:08:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 20:13:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 20:18:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 20:23:06', location_id='loc0', temp_celcius='29'),\n",
       " Row(event_date='03/04/2019 20:28:06', location_id='loc0', temp_celcius='32'),\n",
       " Row(event_date='03/04/2019 20:33:06', location_id='loc0', temp_celcius='35')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = spark.read.format('csv').option('header', 'true').load(file_path)\n",
    "df0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show()    # timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_path_no_header = data_path + '\\\\utilization.csv'\n",
    "# no header > infer a schema\n",
    "df2 = spark.read.format('csv').option('header','false').option('inferschema','true').load(file_path_no_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='03/05/2019 08:06:14', _c1=100, _c2=0.57, _c3=0.51, _c4=47),\n",
       " Row(_c0='03/05/2019 08:11:14', _c1=100, _c2=0.47, _c3=0.62, _c4=43),\n",
       " Row(_c0='03/05/2019 08:16:14', _c1=100, _c2=0.56, _c3=0.57, _c4=62),\n",
       " Row(_c0='03/05/2019 08:21:14', _c1=100, _c2=0.57, _c3=0.56, _c4=50),\n",
       " Row(_c0='03/05/2019 08:26:14', _c1=100, _c2=0.35, _c3=0.46, _c4=43),\n",
       " Row(_c0='03/05/2019 08:31:14', _c1=100, _c2=0.41, _c3=0.58, _c4=48),\n",
       " Row(_c0='03/05/2019 08:36:14', _c1=100, _c2=0.57, _c3=0.35, _c4=58),\n",
       " Row(_c0='03/05/2019 08:41:14', _c1=100, _c2=0.41, _c3=0.4, _c4=58),\n",
       " Row(_c0='03/05/2019 08:46:14', _c1=100, _c2=0.53, _c3=0.35, _c4=62),\n",
       " Row(_c0='03/05/2019 08:51:14', _c1=100, _c2=0.51, _c3=0.6, _c4=45)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(10)   # dont have column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# column rename\n",
    "df2=df2.withColumnRenamed('_c0', 'event_datetime') \\\n",
    "    .withColumnRenamed('_c1', 'server_id')\\\n",
    "    .withColumnRenamed('_c2', 'cpu_utilization')\\\n",
    "    .withColumnRenamed('_c3', 'free_memory')\\\n",
    "    .withColumnRenamed('_c4', 'session_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event_datetime='03/05/2019 08:06:14', server_id=100, cpu_utilization=0.57, free_memory=0.51, session_count=47),\n",
       " Row(event_datetime='03/05/2019 08:11:14', server_id=100, cpu_utilization=0.47, free_memory=0.62, session_count=43)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n",
      "|03/05/2019 08:31:14|      100|           0.41|       0.58|           48|\n",
      "|03/05/2019 08:36:14|      100|           0.57|       0.35|           58|\n",
      "|03/05/2019 08:41:14|      100|           0.41|        0.4|           58|\n",
      "|03/05/2019 08:46:14|      100|           0.53|       0.35|           62|\n",
      "|03/05/2019 08:51:14|      100|           0.51|        0.6|           45|\n",
      "|03/05/2019 08:56:14|      100|           0.32|       0.37|           47|\n",
      "|03/05/2019 09:01:14|      100|           0.62|       0.59|           60|\n",
      "|03/05/2019 09:06:14|      100|           0.66|       0.72|           57|\n",
      "|03/05/2019 09:11:14|      100|           0.54|       0.54|           44|\n",
      "|03/05/2019 09:16:14|      100|           0.29|        0.4|           47|\n",
      "|03/05/2019 09:21:14|      100|           0.43|       0.68|           66|\n",
      "|03/05/2019 09:26:14|      100|           0.49|       0.66|           65|\n",
      "|03/05/2019 09:31:14|      100|           0.64|       0.55|           66|\n",
      "|03/05/2019 09:36:14|      100|           0.42|        0.6|           42|\n",
      "|03/05/2019 09:41:14|      100|           0.55|       0.59|           63|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Load Data into Dataframes : JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "json_df1_path = data_path + '\\\\utilization.json'\n",
    "df1 = spark.read.format('json').load(json_df1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dont have to change colnames >>> json :: specify key value pairs\n",
    "df1.show(2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Basic dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_datetime',\n",
       " 'server_id',\n",
       " 'cpu_utilization',\n",
       " 'free_memory',\n",
       " 'session_count']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns  # attribute of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2_sample = df2.sample(False, fraction = 0.1 )  # WITHOUT REPLACEMENT :: FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 09:11:14|      100|           0.54|       0.54|           44|\n",
      "|03/05/2019 11:01:14|      100|           0.32|       0.56|           53|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_sample.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:54|      124|           0.32|       0.43|           45|\n",
      "|03/05/2019 08:11:31|      110|           0.58|       0.61|           76|\n",
      "|03/05/2019 08:11:41|      116|           0.44|       0.38|           49|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .sort('기준필드')\n",
    "df2_sort = df2_sample.sort('event_datetime')\n",
    "df2_sort.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Filter data with Dataframe API\n",
    "    filter method : df1.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.filter(df0['location_id']=='loc0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.filter(df0['location_id']=='loc0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.filter(df0['location_id']=='loc1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc1|          31|\n",
      "|03/04/2019 19:53:06|       loc1|          26|\n",
      "|03/04/2019 19:58:06|       loc1|          31|\n",
      "|03/04/2019 20:03:06|       loc1|          26|\n",
      "|03/04/2019 20:08:06|       loc1|          28|\n",
      "|03/04/2019 20:13:06|       loc1|          27|\n",
      "|03/04/2019 20:18:06|       loc1|          30|\n",
      "|03/04/2019 20:23:06|       loc1|          28|\n",
      "|03/04/2019 20:28:06|       loc1|          28|\n",
      "|03/04/2019 20:33:06|       loc1|          27|\n",
      "|03/04/2019 20:38:06|       loc1|          30|\n",
      "|03/04/2019 20:43:06|       loc1|          32|\n",
      "|03/04/2019 20:48:06|       loc1|          26|\n",
      "|03/04/2019 20:53:06|       loc1|          30|\n",
      "|03/04/2019 20:58:06|       loc1|          26|\n",
      "|03/04/2019 21:03:06|       loc1|          28|\n",
      "|03/04/2019 21:08:06|       loc1|          27|\n",
      "|03/04/2019 21:13:06|       loc1|          28|\n",
      "|03/04/2019 21:18:06|       loc1|          27|\n",
      "|03/04/2019 21:23:06|       loc1|          26|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.filter(df0['location_id']=='loc1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Aggregated data with Dataframe API\n",
    "    agg method : df1.groupby/orderby.agg({'필드' : 기술통계값})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|location_id|count|\n",
      "+-----------+-----+\n",
      "|     loc196| 1000|\n",
      "|     loc226| 1000|\n",
      "+-----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how many differenct measurements we have for each location\n",
    "df0.groupby('location_id').count().show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.orderBy('location_id').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|     loc196|           29.225|\n",
      "|     loc226|           25.306|\n",
      "+-----------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.groupby('location_id').agg({'temp_celcius':'mean'}).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|max(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|     loc196|               36|\n",
      "|     loc226|               32|\n",
      "+-----------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.groupby('location_id').agg({'temp_celcius':'max'}).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Sample data from Dataframes\n",
    "    df.sample(fraction=0.1, withReplacement=False)\n",
    "    df.groupby().agg({})\n",
    "    df.groupby().agg({}).orderBy()\n",
    "    df.orderBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50248"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0_s1 = df0.sample(fraction=0.1, withReplacement = False)\n",
    "df0_s1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|     loc196|29.242990654205606|\n",
      "|     loc226| 25.36470588235294|\n",
      "+-----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0_s1.groupby('location_id').agg({'temp_celcius':'mean'}).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|       loc0|29.116504854368934|\n",
      "|       loc1|27.808988764044944|\n",
      "+-----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0_s1.groupby('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df0.groupby('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Save data from Dataframes\n",
    "    df.write.csv('***.csv')\n",
    "    df.write.csv('***.json')\n",
    "    ! dir ***.csv\n",
    "    ! head ***.csv/'directoryname'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = 'C:\\\\Users\\\\JIIN\\\\OneDrive\\\\Documents\\\\Github\\\\2021_Python\\\\Spark_SQL\\\\data'\n",
    "file_path = data_path + '\\\\location_temp.csv'\n",
    "df0 = spark.read.format('csv').option('header', 'true').load(file_path)\n",
    "df0.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df0.write.csv('df0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!dir df0.csv   \n",
    "# exclamation mark :: shell command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    df0.csv is not single file >> directory >> files with csv extentions\n",
    "    spark can breack up df into partition subsets\n",
    "    each partition has its own file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! head df0.csv/''directoryname''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df0.write.json('df0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dir df0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! head df0.json/''directoryname''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "저장시 오류나는 경우 \n",
    "./pyspark --packages com.databricks:spark-csv_2.10:1.4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL for DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark session as global variable\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Spark SQL Query Dataframes\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\JIIN\\\\OneDrive\\\\Documents\\\\Github\\\\2021_Python\\\\Spark_SQL\\\\data'\n",
    "json_df1_path = data_path + '\\\\utilization.json'\n",
    "df = spark.read.format('json').load(json_df1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dataframes with SQL\n",
    "    1) df.createOrReplaceTempView('뷰이름') -- 뷰생성\n",
    "    2) spark.sql('SQL Statement') -- sql Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#SQL > have to create temporary view\n",
    "df.createOrReplaceTempView('utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select * from utilization limit 10')\n",
    "df_sql.show(2)\n",
    "# same structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      100|           47|\n",
      "|      100|           43|\n",
      "|      100|           62|\n",
      "+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select server_id, session_count from utilization limit 10')\n",
    "df_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid| sc|\n",
      "+---+---+\n",
      "|100| 47|\n",
      "|100| 43|\n",
      "|100| 62|\n",
      "+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select server_id AS sid, session_count AS sc from utilization limit 10')\n",
    "df_sql.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Filtering Dataframes with SQL\n",
    "    spark.sql('SQL Query with WHERE clause')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select * from utilization where server_id =100')\n",
    "df_sql.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3078"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql = spark.sql('select server_id from utilization where session_count >100')\n",
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple conditions\n",
    "df_sql = spark.sql('select server_id from utilization where session_count >70 and server_id =120')\n",
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple lines ::  \\\n",
    "df_sql = spark.sql('SELECT server_id FROM utilization \\\n",
    "                    WHERE session_count >70 AND server_id =120 \\\n",
    "                    ORDER BY session_count DESC')\n",
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Aggregating Dataframes with SQL\n",
    "    spark.sql('SQL Query with group by & aggregation functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  239659|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select count(*) \\\n",
    "                    from utilization \\\n",
    "                    where session_count > 70')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      101|    9808|\n",
      "|      113|    9418|\n",
      "|      145|    9304|\n",
      "|      103|    8744|\n",
      "|      102|    8586|\n",
      "|      133|    8583|\n",
      "|      108|    8375|\n",
      "|      149|    8288|\n",
      "|      137|    8248|\n",
      "|      148|    8027|\n",
      "|      123|    7918|\n",
      "|      118|    7913|\n",
      "|      112|    7425|\n",
      "|      139|    7383|\n",
      "|      104|    7366|\n",
      "|      142|    7084|\n",
      "|      121|    7084|\n",
      "|      146|    7072|\n",
      "|      126|    6365|\n",
      "|      144|    6220|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select server_id, count(*) \\\n",
    "                    from utilization \\\n",
    "                    where session_count > 70\\\n",
    "                    group by server_id\\\n",
    "                    order by count(*) desc')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+----------------------------+\n",
      "|server_id|min(session_count)|max(session_count)|round(avg(session_count), 2)|\n",
      "+---------+------------------+------------------+----------------------------+\n",
      "|      101|                71|               105|                       87.67|\n",
      "|      113|                71|               103|                       86.96|\n",
      "|      145|                71|               103|                       86.98|\n",
      "|      103|                71|               101|                       85.76|\n",
      "|      102|                71|               101|                       85.71|\n",
      "|      133|                71|               100|                       85.47|\n",
      "|      108|                71|               100|                       85.12|\n",
      "|      149|                71|                99|                       84.96|\n",
      "|      137|                71|                99|                       85.01|\n",
      "|      148|                71|                99|                        84.7|\n",
      "|      123|                71|                98|                       84.53|\n",
      "|      118|                71|                98|                       84.66|\n",
      "|      112|                71|                97|                       83.55|\n",
      "|      139|                71|                96|                       83.33|\n",
      "|      104|                71|                96|                       83.35|\n",
      "|      121|                71|                95|                       82.89|\n",
      "|      142|                71|                95|                        82.9|\n",
      "|      146|                71|                95|                       82.95|\n",
      "|      126|                71|                93|                       81.56|\n",
      "|      144|                71|                92|                       81.38|\n",
      "+---------+------------------+------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('select server_id, min(session_count), max(session_count), round(avg(session_count),2) \\\n",
    "                    from utilization \\\n",
    "                    where session_count > 70\\\n",
    "                    group by server_id\\\n",
    "                    order by count(*) desc')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Joining Dataframes with SQL\n",
    "    spark.sql('SQL Query with Join & On|Where')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = 'C:\\\\Users\\\\JIIN\\\\OneDrive\\\\Documents\\\\Github\\\\2021_Python\\\\Spark_SQL\\\\data'\n",
    "json_df1_path = data_path + '\\\\utilization.json'\n",
    "df_util = spark.read.format('json').load(json_df1_path)\n",
    "df_util.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView('utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "csv_df_path = data_path + '\\\\server_name.csv'\n",
    "df_server = spark.read.csv(csv_df_path, header =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "+---------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_server.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_server.createOrReplaceTempView('servername')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|server_id|\n",
      "+---------+\n",
      "|      100|\n",
      "|      101|\n",
      "|      102|\n",
      "|      103|\n",
      "|      104|\n",
      "|      105|\n",
      "|      106|\n",
      "|      107|\n",
      "|      108|\n",
      "|      109|\n",
      "|      110|\n",
      "|      111|\n",
      "|      112|\n",
      "|      113|\n",
      "|      114|\n",
      "|      115|\n",
      "|      116|\n",
      "|      117|\n",
      "|      118|\n",
      "|      119|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count = spark.sql('select distinct server_id \\\n",
    "                    from utilization \\\n",
    "                     order by server_id')\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(server_id)|max(server_id)|\n",
      "+--------------+--------------+\n",
      "|           100|           149|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select min(server_id), max(server_id)\\\n",
    "          from utilization').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|server_id|server_name|session_count|\n",
      "+---------+-----------+-------------+\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           43|\n",
      "|      100| 100 Server|           62|\n",
      "|      100| 100 Server|           50|\n",
      "|      100| 100 Server|           43|\n",
      "|      100| 100 Server|           48|\n",
      "|      100| 100 Server|           58|\n",
      "|      100| 100 Server|           58|\n",
      "|      100| 100 Server|           62|\n",
      "|      100| 100 Server|           45|\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           60|\n",
      "|      100| 100 Server|           57|\n",
      "|      100| 100 Server|           44|\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           66|\n",
      "|      100| 100 Server|           65|\n",
      "|      100| 100 Server|           66|\n",
      "|      100| 100 Server|           42|\n",
      "|      100| 100 Server|           63|\n",
      "+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = spark.sql('select u.server_id, s.server_name, u.session_count\\\n",
    "                    from utilization u \\\n",
    "                    inner join servername s \\\n",
    "                    on s.server_id = u.server_id')\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Eliminating duplicates in Dataframes\n",
    "    df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* sc :: global variable to access to the Sparck Context\n",
    "* create parallelized data structure \n",
    "    - spark can breack up dataframes into partition subsets\n",
    "    - sql: create it manually so we have to specifying parallelized explicitly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc =SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_dup = sc.parallelize([Row(server_name='101 Server',cpu_utilization =85, session_count=80),\\\n",
    "                        Row(server_name='101 Server',cpu_utilization =80, session_count=90),\\\n",
    "                        Row(server_name='102 Server',cpu_utilization =85, session_count=80),\\\n",
    "                        Row(server_name='102 Server',cpu_utilization =85, session_count=80)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_dup.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_dup.drop_duplicates('server_name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Working with NA values in Dataframes \n",
    "    df.withColumn('추가할필드명', function/type)\n",
    "    df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80), \\\n",
    "                             Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                             Row(server_name='102 Server', cpu_utilization=85, session_count=40),\n",
    "                             Row(server_name='103 Server', cpu_utilization=70, session_count=80),\n",
    "                             Row(server_name='104 Server', cpu_utilization=60, session_count=80)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# adding column\n",
    "# literal value\n",
    "df_na = df.withColumn('na_col', lit(None).cast(StringType()))\n",
    "df_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_na.fillna('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# union all rows\n",
    "df2 = df_na.fillna('A').union(df_na)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('na_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select * from na_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select * from na_table where na_col is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select * from na_table where na_col is not null').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### EDA with Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = 'C:\\\\Users\\\\JIIN\\\\OneDrive\\\\Documents\\\\Github\\\\2021_Python\\\\Spark_SQL\\\\data'\n",
    "json_df2_path = data_path + '\\\\utilization.json'\n",
    "df_util= spark.read.format('json').load(json_df2_path)\n",
    "df_util.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView('utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "|summary|    cpu_utilization|     event_datetime|        free_memory|         server_id|     session_count|\n",
      "+-------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "|  count|             500000|             500000|             500000|            500000|            500000|\n",
      "|   mean| 0.6205177399999957|               null|0.37912809999999864|             124.5|          69.59616|\n",
      "| stddev|0.15875173872912945|               null|0.15830931278376276|14.430884120553191|14.850676696352851|\n",
      "|    min|               0.22|03/05/2019 08:06:14|                0.0|               100|                32|\n",
      "|    max|                1.0|04/09/2019 01:22:46|               0.78|               149|               105|\n",
      "+-------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.describe().show() #summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4704771573080754"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.stat.corr('cpu_utilization', 'free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5008320848876573"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.stat.corr('session_count' ,'free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "| server_id_freqItems|session_count_freqItems|\n",
      "+--------------------+-----------------------+\n",
      "|[146, 137, 101, 1...|   [92, 101, 83, 95,...|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.stat.freqItems(('server_id','session_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24964"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset \n",
    "# df.sample() method returns dataframe itself\n",
    "df_util_sample = df_util.sample(fraction=0.05, withReplacement=False)\n",
    "df_util_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+\n",
      "|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n",
      "+--------------------+--------------------+-----------------------+\n",
      "|                0.22|                 1.0|    0.15875173872912945|\n",
      "+--------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select min(cpu_utilization), max(cpu_utilization),stddev(cpu_utilization)\\\n",
    "          from utilization').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|      112|                0.52|                0.92|    0.11528867845082576|\n",
      "|      113|                0.58|                0.98|    0.11544345150353687|\n",
      "|      130|                0.35|                0.75|    0.11568834774245991|\n",
      "|      126|                0.48|                0.88|    0.11542612970702058|\n",
      "|      149|                0.54|                0.94|    0.11543517500295467|\n",
      "|      110|                0.35|                0.75|    0.11533251724450215|\n",
      "|      136|                0.41|                 0.8|    0.11597405743182258|\n",
      "|      144|                0.47|                0.87|    0.11478654960489501|\n",
      "|      119|                0.22|                0.62|    0.11516031929842008|\n",
      "|      116|                 0.3|                 0.7|    0.11506079722349302|\n",
      "|      145|                0.58|                0.98|    0.11476448868786197|\n",
      "|      124|                0.24|                0.64|    0.11579377614906383|\n",
      "|      143|                0.26|                0.66|    0.11616596078044727|\n",
      "|      107|                0.45|                0.85|    0.11597417369783877|\n",
      "|      146|                 0.5|                 0.9|    0.11488129439634719|\n",
      "|      103|                0.56|                0.96|    0.11617507884178278|\n",
      "|      139|                0.51|                0.91|    0.11519660023052102|\n",
      "|      138|                0.24|                0.64|     0.1155955860444133|\n",
      "|      114|                0.33|                0.73|    0.11510268816097273|\n",
      "|      115|                0.44|                0.84|    0.11569664615014985|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select server_id,min(cpu_utilization), max(cpu_utilization),stddev(cpu_utilization)\\\n",
    "          from utilization \\\n",
    "          group by server_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|server_id|bucket|\n",
      "+---------+------+\n",
      "|      100|     5|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     5|\n",
      "|      100|     3|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     5|\n",
      "|      100|     3|\n",
      "|      100|     6|\n",
      "|      100|     6|\n",
      "|      100|     5|\n",
      "|      100|     2|\n",
      "|      100|     4|\n",
      "|      100|     4|\n",
      "|      100|     6|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select server_id,\\\n",
    "          floor(cpu_utilization*100/10) bucket\\\n",
    "          from utilization').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|count(1)|bucket|\n",
      "+--------+------+\n",
      "|    8186|     2|\n",
      "|   37029|     3|\n",
      "|   68046|     4|\n",
      "|  104910|     5|\n",
      "|  116725|     6|\n",
      "|   88242|     7|\n",
      "|   56598|     8|\n",
      "|   20207|     9|\n",
      "|      57|    10|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*),\\\n",
    "          floor(cpu_utilization*100/10) bucket\\\n",
    "          from utilization\\\n",
    "          group by bucket\\\n",
    "          order by bucket').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeseries analysis with Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|      112|                0.52|                0.92|    0.11528867845082576|\n",
      "|      113|                0.58|                0.98|    0.11544345150353687|\n",
      "|      130|                0.35|                0.75|    0.11568834774245991|\n",
      "|      126|                0.48|                0.88|    0.11542612970702058|\n",
      "|      149|                0.54|                0.94|    0.11543517500295467|\n",
      "|      110|                0.35|                0.75|    0.11533251724450215|\n",
      "|      136|                0.41|                 0.8|    0.11597405743182258|\n",
      "|      144|                0.47|                0.87|    0.11478654960489501|\n",
      "|      119|                0.22|                0.62|    0.11516031929842008|\n",
      "|      116|                 0.3|                 0.7|    0.11506079722349302|\n",
      "|      145|                0.58|                0.98|    0.11476448868786197|\n",
      "|      124|                0.24|                0.64|    0.11579377614906383|\n",
      "|      143|                0.26|                0.66|    0.11616596078044727|\n",
      "|      107|                0.45|                0.85|    0.11597417369783877|\n",
      "|      146|                 0.5|                 0.9|    0.11488129439634719|\n",
      "|      103|                0.56|                0.96|    0.11617507884178278|\n",
      "|      139|                0.51|                0.91|    0.11519660023052102|\n",
      "|      138|                0.24|                0.64|     0.1155955860444133|\n",
      "|      114|                0.33|                0.73|    0.11510268816097273|\n",
      "|      115|                0.44|                0.84|    0.11569664615014985|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select server_id,min(cpu_utilization), max(cpu_utilization),stddev(cpu_utilization)\\\n",
    "          from utilization \\\n",
    "          group by server_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:06:34|      112|           0.71|0.7153870000000067|\n",
      "|03/05/2019 08:11:34|      112|           0.78|0.7153870000000067|\n",
      "|03/05/2019 08:16:34|      112|           0.87|0.7153870000000067|\n",
      "|03/05/2019 08:21:34|      112|           0.82|0.7153870000000067|\n",
      "|03/05/2019 08:26:34|      112|           0.62|0.7153870000000067|\n",
      "|03/05/2019 08:31:34|      112|            0.9|0.7153870000000067|\n",
      "|03/05/2019 08:36:34|      112|           0.89|0.7153870000000067|\n",
      "|03/05/2019 08:41:34|      112|           0.81|0.7153870000000067|\n",
      "|03/05/2019 08:46:34|      112|           0.88|0.7153870000000067|\n",
      "|03/05/2019 08:51:34|      112|           0.89|0.7153870000000067|\n",
      "|03/05/2019 08:56:34|      112|           0.84|0.7153870000000067|\n",
      "|03/05/2019 09:01:34|      112|           0.71|0.7153870000000067|\n",
      "|03/05/2019 09:06:34|      112|           0.85|0.7153870000000067|\n",
      "|03/05/2019 09:11:34|      112|           0.72|0.7153870000000067|\n",
      "|03/05/2019 09:16:34|      112|           0.54|0.7153870000000067|\n",
      "|03/05/2019 09:21:34|      112|           0.58|0.7153870000000067|\n",
      "|03/05/2019 09:26:34|      112|           0.73|0.7153870000000067|\n",
      "|03/05/2019 09:31:34|      112|           0.86|0.7153870000000067|\n",
      "|03/05/2019 09:36:34|      112|           0.63|0.7153870000000067|\n",
      "|03/05/2019 09:41:34|      112|           0.75|0.7153870000000067|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# windowing function :: compare values within the specific group\n",
    "sql_window=spark.sql('select event_datetime, server_id, cpu_utilization, \\\n",
    "            avg(cpu_utilization) over (partition by server_id) avg_server_util\\\n",
    "          from utilization ')\n",
    "sql_window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|   delta_server_util|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|03/05/2019 08:06:34|      112|           0.71|0.7153870000000067|-0.00538700000000...|\n",
      "|03/05/2019 08:11:34|      112|           0.78|0.7153870000000067| 0.06461299999999337|\n",
      "|03/05/2019 08:16:34|      112|           0.87|0.7153870000000067| 0.15461299999999334|\n",
      "|03/05/2019 08:21:34|      112|           0.82|0.7153870000000067|  0.1046129999999933|\n",
      "|03/05/2019 08:26:34|      112|           0.62|0.7153870000000067|-0.09538700000000666|\n",
      "|03/05/2019 08:31:34|      112|            0.9|0.7153870000000067| 0.18461299999999337|\n",
      "|03/05/2019 08:36:34|      112|           0.89|0.7153870000000067| 0.17461299999999336|\n",
      "|03/05/2019 08:41:34|      112|           0.81|0.7153870000000067|  0.0946129999999934|\n",
      "|03/05/2019 08:46:34|      112|           0.88|0.7153870000000067| 0.16461299999999335|\n",
      "|03/05/2019 08:51:34|      112|           0.89|0.7153870000000067| 0.17461299999999336|\n",
      "|03/05/2019 08:56:34|      112|           0.84|0.7153870000000067| 0.12461299999999331|\n",
      "|03/05/2019 09:01:34|      112|           0.71|0.7153870000000067|-0.00538700000000...|\n",
      "|03/05/2019 09:06:34|      112|           0.85|0.7153870000000067| 0.13461299999999332|\n",
      "|03/05/2019 09:11:34|      112|           0.72|0.7153870000000067|0.004612999999993317|\n",
      "|03/05/2019 09:16:34|      112|           0.54|0.7153870000000067|-0.17538700000000662|\n",
      "|03/05/2019 09:21:34|      112|           0.58|0.7153870000000067| -0.1353870000000067|\n",
      "|03/05/2019 09:26:34|      112|           0.73|0.7153870000000067|0.014612999999993326|\n",
      "|03/05/2019 09:31:34|      112|           0.86|0.7153870000000067| 0.14461299999999333|\n",
      "|03/05/2019 09:36:34|      112|           0.63|0.7153870000000067|-0.08538700000000665|\n",
      "|03/05/2019 09:41:34|      112|           0.75|0.7153870000000067|0.034612999999993344|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_window2=spark.sql('select event_datetime, server_id, cpu_utilization, \\\n",
    "            avg(cpu_utilization) over (partition by server_id) avg_server_util,\\\n",
    "            cpu_utilization - avg(cpu_utilization) over (partition by server_id) delta_server_util\\\n",
    "          from utilization ')\n",
    "sql_window2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:06:34|      112|           0.71|             0.745|\n",
      "|03/05/2019 08:11:34|      112|           0.78|0.7866666666666666|\n",
      "|03/05/2019 08:16:34|      112|           0.87|0.8233333333333333|\n",
      "|03/05/2019 08:21:34|      112|           0.82|              0.77|\n",
      "|03/05/2019 08:26:34|      112|           0.62|0.7799999999999999|\n",
      "|03/05/2019 08:31:34|      112|            0.9|0.8033333333333333|\n",
      "|03/05/2019 08:36:34|      112|           0.89|0.8666666666666667|\n",
      "|03/05/2019 08:41:34|      112|           0.81|              0.86|\n",
      "|03/05/2019 08:46:34|      112|           0.88|              0.86|\n",
      "|03/05/2019 08:51:34|      112|           0.89|              0.87|\n",
      "|03/05/2019 08:56:34|      112|           0.84|0.8133333333333334|\n",
      "|03/05/2019 09:01:34|      112|           0.71|0.7999999999999999|\n",
      "|03/05/2019 09:06:34|      112|           0.85|0.7600000000000001|\n",
      "|03/05/2019 09:11:34|      112|           0.72|0.7033333333333333|\n",
      "|03/05/2019 09:16:34|      112|           0.54|0.6133333333333333|\n",
      "|03/05/2019 09:21:34|      112|           0.58|0.6166666666666667|\n",
      "|03/05/2019 09:26:34|      112|           0.73|0.7233333333333333|\n",
      "|03/05/2019 09:31:34|      112|           0.86|0.7399999999999999|\n",
      "|03/05/2019 09:36:34|      112|           0.63|0.7466666666666667|\n",
      "|03/05/2019 09:41:34|      112|           0.75|0.6999999999999998|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SLIDING WINDOW  >  ROWS BETWEEN * AND *\n",
    "# neighborhood of a row\n",
    "# average of the value right before, current row and following row\n",
    "sql_window3 = spark.sql ('select event_datetime, server_id, cpu_utilization, \\\n",
    "                         avg(cpu_utilization) over (partition by server_id order by event_datetime \\\n",
    "                                             rows between 1 preceding and 1 following) avg_server_util\\\n",
    "                        from utilization')\n",
    "sql_window3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic  ML with Dataframes : Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTERING\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector :: single array \n",
    "# (single data structure that holds all the values from a particular row that ml algorithms will be looking at)\n",
    "# create vector\n",
    "vector = VectorAssembler( inputCols = ['cpu_utilization','free_memory','session_count'] , \n",
    "                         # name of the columns you are interested in \n",
    "                         outputCol='features')\n",
    "                        # outputparameter name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|        features|\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|[0.57,0.51,47.0]|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|[0.47,0.62,43.0]|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|[0.56,0.57,62.0]|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|[0.57,0.56,50.0]|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|[0.35,0.46,43.0]|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|[0.41,0.58,48.0]|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|[0.57,0.35,58.0]|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58| [0.41,0.4,58.0]|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|[0.53,0.35,62.0]|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45| [0.51,0.6,45.0]|\n",
      "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|[0.32,0.37,47.0]|\n",
      "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|[0.62,0.59,60.0]|\n",
      "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|[0.66,0.72,57.0]|\n",
      "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|[0.54,0.54,44.0]|\n",
      "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47| [0.29,0.4,47.0]|\n",
      "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|[0.43,0.68,66.0]|\n",
      "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|[0.49,0.66,65.0]|\n",
      "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|[0.64,0.55,66.0]|\n",
      "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42| [0.42,0.6,42.0]|\n",
      "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|[0.55,0.59,63.0]|\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe using tranform method\n",
    "vcluster_df = vector.transform(df_util)\n",
    "vcluster_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ml algorithms in spark expect the input data to be in a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(3) #number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = kmeans.setSeed(1) # seed for its random value generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel = kmeans.fit(vcluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.52047775,  0.47836303, 51.79927162]),\n",
       " array([ 0.62881549,  0.37094643, 70.43030159]),\n",
       " array([ 0.71931575,  0.28104316, 88.23965784])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmodel.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic  ML with Dataframes : Linear Regression\n",
    "    prediction about one variable using knowledge about another variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector\n",
    "vector2 = VectorAssembler( inputCols = ['cpu_utilization'],\n",
    "                         # name of the columns you are interested in \n",
    "                         outputCol='features')\n",
    "                        # outputparameter name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vutil = vector2.transform(df_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|features|\n",
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|  [0.57]|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|  [0.47]|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|  [0.56]|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|  [0.57]|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|  [0.35]|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|  [0.41]|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|  [0.57]|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|  [0.41]|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|  [0.53]|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|  [0.51]|\n",
      "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|  [0.32]|\n",
      "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|  [0.62]|\n",
      "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|  [0.66]|\n",
      "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|  [0.54]|\n",
      "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|  [0.29]|\n",
      "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|  [0.43]|\n",
      "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|  [0.49]|\n",
      "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|  [0.64]|\n",
      "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|  [0.42]|\n",
      "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|  [0.55]|\n",
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vutil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr  = LinearRegression(featuresCol = 'features', labelCol ='session_count')\n",
    "lrModel = lr.fit(df_vutil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear Regression model specified by 2 thinngs :: coefficient and intercept ax+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([47.024])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.41695103556804"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.837990225931815"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the error occurs when you fit the model\n",
    "lrModel.summary.rootMeanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
